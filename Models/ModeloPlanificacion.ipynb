{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of inputs (game states, one-hot encoded)\n",
    "X = tf.placeholder(tf.float32, [None, 13, 26, 9], name=\"X\") # type tf.float32 is needed for the rest of operations\n",
    "\n",
    "# Batch of outputs (correct predictions of number of actions)\n",
    "Y_corr = tf.placeholder(tf.float32, [None, 1], name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture\n",
    "\n",
    "# Flatten input\n",
    "X_flat = tf.layers.flatten(X)\n",
    "\n",
    "# -First Layer - Receives inputs\n",
    "\n",
    "layer_1 = tf.layers.dense(inputs = X_flat,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.relu,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  use_bias = True,\n",
    "                                  name=\"layer_1\")\n",
    "\n",
    "# Dropout probability\n",
    "dropout_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Add dropout to prevent overfitting\n",
    "layer_1 = tf.layers.dropout(layer_1, rate=dropout_prob)\n",
    "\n",
    "with tf.variable_scope(\"layer_1\", reuse=True): # Get weights and bias for histogram\n",
    "    weights_1 = tf.get_variable(\"kernel\")\n",
    "    bias_1 = tf.get_variable(\"bias\")\n",
    "    \n",
    "# -Second Layer-\n",
    "\n",
    "layer_2 = tf.layers.dense(inputs = layer_1,\n",
    "                                  units = 16,\n",
    "                                  activation = tf.nn.relu,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  use_bias = True,\n",
    "                                  name=\"layer_2\")\n",
    "\n",
    "# Add dropout to prevent overfitting\n",
    "layer_2 = tf.layers.dropout(layer_2, rate=dropout_prob)\n",
    "\n",
    "# -Output Layer-\n",
    "\n",
    "output = tf.layers.dense(inputs = layer_2,\n",
    "                                  units = 1,\n",
    "                                  activation = None,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  use_bias = True,\n",
    "                                  name=\"output\")\n",
    "\n",
    "with tf.variable_scope(\"output\", reuse=True): # Get weights and bias for histogram\n",
    "    weights_output = tf.get_variable(\"kernel\")\n",
    "    bias_output = tf.get_variable(\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "loss = tf.reduce_mean(tf.square(output - Y_corr), name=\"loss\") # Quadratic loss\n",
    "\n",
    "alfa = tf.placeholder(tf.float64, name='alfa') # Mejor valor alfa=0.001\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=alfa, name=\"optimizer\") # Mejor optimizer\n",
    "train_op = optimizer.minimize(loss, name=\"train_op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "\n",
    "# Summaries\n",
    "train_loss_sum = tf.summary.scalar('train_loss', loss)\n",
    "\n",
    "test_loss_sum = tf.summary.scalar('test_loss', loss) # Validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---MAE TRAIN---\n",
      "\n",
      "\n",
      "Model: 1.039887220668849\n",
      "Baseline: 6.7887650132224895\n",
      "\n",
      "\n",
      "---MAE VALIDATION--\n",
      "\n",
      "\n",
      "Model: 6.167267234085233\n",
      "Baseline: 7.099567260502611\n"
     ]
    }
   ],
   "source": [
    "# Session\n",
    "\n",
    "# Load Training Dataset\n",
    "dataset = np.load('datasets/dataset_train_3.npz')\n",
    "dataset_x = dataset['X']\n",
    "dataset_y = dataset['Y']\n",
    "\n",
    "# Load Validation Dataset\n",
    "test_dataset = np.load(\"datasets/dataset_test.npz\") \n",
    "test_dataset_x = test_dataset['X']\n",
    "test_dataset_y = test_dataset['Y']\n",
    "test_dataset_y_resh = np.reshape(test_dataset_y, (-1, 1)) \n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "# Minibatch size\n",
    "batch_size = 128 # Best batch size\n",
    "\n",
    "drop_val = 0.7 # 0.7 and 0.5 best vals\n",
    " \n",
    "# Learning rates to test\n",
    "dropout_vals = [0] \n",
    "\n",
    "for drop_val in dropout_vals:\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"ModeloPlanificacion_log/arch=[512, 16]\")\n",
    "        writer.add_graph(tf.get_default_graph())\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer()) # Initialize all variables\n",
    "\n",
    "        rkf = RepeatedKFold(n_splits=int(len(dataset_y) // batch_size),\n",
    "                      n_repeats=num_epochs) # Get randomized indexes for minibatches\n",
    "\n",
    "        # Feed Dict for validation (uses validation set)\n",
    "        data_dict_test = {X:test_dataset_x, Y_corr:test_dataset_y_resh, dropout_prob:0.0}\n",
    "        \n",
    "        # Train the model\n",
    "        \n",
    "        it = 0\n",
    "        \n",
    "        for _, batch_index in rkf.split(dataset_y):\n",
    "            batch_x = np.take(dataset_x, batch_index, axis=0) # Obtain current training batch\n",
    "            batch_y = np.take(dataset_y, batch_index)\n",
    "            batch_y = np.reshape(batch_y, (-1, 1))\n",
    "            \n",
    "            data_dict = {X:batch_x, Y_corr:batch_y, alfa:0.001, dropout_prob:drop_val}\n",
    "           \n",
    "            sess.run(train_op, feed_dict=data_dict) # Execute one training step\n",
    "            \n",
    "            # Periodically check losses\n",
    "            if it % 5 == 0:                \n",
    "                sum2 = sess.run(test_loss_sum, feed_dict=data_dict_test)\n",
    "                writer.add_summary(sum2, it)\n",
    "                sum1 = sess.run(train_loss_sum, feed_dict=data_dict)\n",
    "                writer.add_summary(sum1, it)\n",
    "        \n",
    "\n",
    "            it += 1\n",
    "\n",
    "\n",
    "        # --- After training ---\n",
    "        \n",
    "        # Compute MAE\n",
    "            \n",
    "        dataset_y_reshaped = np.reshape(dataset_y, (-1, 1))   \n",
    "        data_dict = {X:dataset_x, dropout_prob:0.0}\n",
    "        \n",
    "        y_pred = sess.run(output, feed_dict=data_dict)\n",
    "        \n",
    "        mae_base = mean_absolute_error(dataset_y, np.repeat(np.mean(dataset_y), dataset_y.shape[0]))\n",
    "        mae_model = mean_absolute_error(dataset_y, y_pred)\n",
    "        \n",
    "        print(\"\\n\\n---MAE TRAIN---\\n\\n\")\n",
    "        print(\"Model:\", mae_model)\n",
    "        print(\"Baseline:\", mae_base)\n",
    "        \n",
    "        # ----- VALIDATION -----\n",
    "\n",
    "        # Compute predictions\n",
    "        data_dict = {X:test_dataset_x, dropout_prob:0.0}\n",
    "        test_pred = sess.run(output, feed_dict=data_dict)\n",
    "        \n",
    "        mae_base_test = mean_absolute_error(test_dataset_y, np.repeat(np.mean(dataset_y), test_dataset_y.shape[0]))\n",
    "        mae_model_test = mean_absolute_error(test_dataset_y, test_pred)\n",
    "        \n",
    "        print(\"\\n\\n---MAE VALIDATION--\\n\\n\")\n",
    "        print(\"Model:\", mae_model_test)\n",
    "        print(\"Baseline:\", mae_base_test)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
