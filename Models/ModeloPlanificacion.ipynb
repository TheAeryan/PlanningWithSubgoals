{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of inputs (game states, one-hot encoded)\n",
    "X = tf.placeholder(tf.float32, [None, 13, 26, 9], name=\"X\") # type tf.float32 is needed for the rest of operations\n",
    "\n",
    "# Batch of outputs (correct predictions of number of actions)\n",
    "Y_corr = tf.placeholder(tf.float32, [None, 1], name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture\n",
    "\n",
    "# Flatten input\n",
    "X_flat = tf.layers.flatten(X)\n",
    "\n",
    "# First Layer - Receives inputs\n",
    "layer_1 = tf.layers.dense(inputs = X_flat,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.relu,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  use_bias = True,\n",
    "                                  name=\"layer_1\")\n",
    "\n",
    "# Dropout probability\n",
    "dropout_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Add dropout to prevent overfitting\n",
    "layer_1 = tf.layers.dropout(layer_1, rate=dropout_prob)\n",
    "\n",
    "with tf.variable_scope(\"layer_1\", reuse=True): # Get weights and bias for histogram\n",
    "    weights_1 = tf.get_variable(\"kernel\")\n",
    "    bias_1 = tf.get_variable(\"bias\")\n",
    "    \n",
    "# Second Layer\n",
    "layer_2 = tf.layers.dense(inputs = layer_1,\n",
    "                                  units = 32,\n",
    "                                  activation = tf.nn.relu,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  use_bias = True,\n",
    "                                  name=\"layer_2\")\n",
    "\n",
    "# Add dropout to prevent overfitting\n",
    "layer_2 = tf.layers.dropout(layer_2, rate=dropout_prob)\n",
    "\n",
    "# Output Layer\n",
    "output = tf.layers.dense(inputs = layer_2,\n",
    "                                  units = 1,\n",
    "                                  activation = None,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  use_bias = True,\n",
    "                                  name=\"output\")\n",
    "\n",
    "with tf.variable_scope(\"output\", reuse=True): # Get weights and bias for histogram\n",
    "    weights_output = tf.get_variable(\"kernel\")\n",
    "    bias_output = tf.get_variable(\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "loss = tf.reduce_mean(tf.square(output - Y_corr), name=\"loss\") # Quadratic loss\n",
    "\n",
    "\"\"\"learning_rate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=5000,\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\"\"\" # Funciona peor\n",
    "\n",
    "alfa = tf.placeholder(tf.float64, name='alfa') # Mejor valor alfa=0.001\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=alfa, name=\"optimizer\")\n",
    "train_op = optimizer.minimize(loss, name=\"train_op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "\n",
    "# Summaries\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.histogram(\"weigths_1\", weights_1)\n",
    "tf.summary.histogram(\"bias_1\", bias_1)\n",
    "tf.summary.histogram(\"weigths_output\", weights_output)\n",
    "tf.summary.histogram(\"bias_output\", bias_output)\n",
    "\n",
    "merged_summary = tf.summary.merge_all() # Operación para obtener todos los valores de los summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---MAE TRAIN---\n",
      "\n",
      "\n",
      "Model: 1.303486420049909\n",
      "Baseline: 6.7887650132224895\n",
      "\n",
      "\n",
      "---MAE VALIDATION--\n",
      "\n",
      "\n",
      "Model: 5.786691986665093\n",
      "Baseline: 7.099567260502611\n"
     ]
    }
   ],
   "source": [
    "# Session\n",
    "\n",
    "# Load Dataset\n",
    "dataset = np.load('datasets/dataset_train_3.npz')\n",
    "dataset_x = dataset['X']\n",
    "dataset_y = dataset['Y']\n",
    "\n",
    "# dataset_x.fill(1) # QUITAR DESPUÉS\n",
    "# dataset_y.fill(20)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 80\n",
    "# Minibatch size\n",
    "batch_size = 128\n",
    "\n",
    "# Learning rates to test\n",
    "alfas = [0.001]\n",
    "\n",
    "for this_alfa in alfas:\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"ModeloPlanificacion_log/prueba_validacion\")\n",
    "        writer.add_graph(tf.get_default_graph())\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer()) # Initialize all variables\n",
    "\n",
    "        rkf = RepeatedKFold(n_splits=int(len(dataset_y) // batch_size),\n",
    "                      n_repeats=num_epochs) # Get randomized indexes for minibatches\n",
    "\n",
    "        it = 0\n",
    "\n",
    "        for _, batch_index in rkf.split(dataset_y):\n",
    "            batch_x = np.take(dataset_x, batch_index, axis=0) # Obtain current batch\n",
    "            batch_y = np.take(dataset_y, batch_index)\n",
    "            batch_y = np.reshape(batch_y, (-1, 1))\n",
    "\n",
    "            data_dict = {X:batch_x, Y_corr:batch_y, alfa:0.002, dropout_prob:0.7}\n",
    "\n",
    "            summary = sess.run(merged_summary, feed_dict=data_dict) # Get Summaries\n",
    "            writer.add_summary(summary, it) # Write it to log in disk\n",
    "\n",
    "            sess.run(train_op, feed_dict=data_dict) # Execute one training step\n",
    "\n",
    "            it += 1\n",
    "\n",
    "\n",
    "        # --- After training ---\n",
    "        # Compute MAE\n",
    "            \n",
    "        dataset_y_reshaped = np.reshape(dataset_y, (-1, 1))   \n",
    "        data_dict = {X:dataset_x, dropout_prob:0.0}\n",
    "        \n",
    "        y_pred = sess.run(output, feed_dict=data_dict)\n",
    "        \n",
    "        mae_base = mean_absolute_error(dataset_y, np.repeat(np.mean(dataset_y), dataset_y.shape[0]))\n",
    "        mae_model = mean_absolute_error(dataset_y, y_pred)\n",
    "        \n",
    "        print(\"\\n\\n---MAE TRAIN---\\n\\n\")\n",
    "        print(\"Model:\", mae_model)\n",
    "        print(\"Baseline:\", mae_base)\n",
    "        \n",
    "        #mae_summary = tf.summary.scalar('MAE', MAE) # Summaries\n",
    "        #model_mae = sess.run(mae_summary, feed_dict=data_dict)\n",
    "        #writer.add_summary(model_mae)\n",
    "        \n",
    "        # ----- VALIDATION -----\n",
    "        \n",
    "        # Load dataset\n",
    "        test_dataset = np.load(\"datasets/dataset_test.npz\") \n",
    "        test_dataset_x = test_dataset['X']\n",
    "        test_dataset_y = test_dataset['Y']\n",
    "\n",
    "        # Compute predictions\n",
    "        data_dict = {X:test_dataset_x, dropout_prob:0.0}\n",
    "        test_pred = sess.run(output, feed_dict=data_dict)\n",
    "        \n",
    "        mae_base_test = mean_absolute_error(test_dataset_y, np.repeat(np.mean(dataset_y), test_dataset_y.shape[0]))\n",
    "        mae_model_test = mean_absolute_error(test_dataset_y, test_pred)\n",
    "        \n",
    "        print(\"\\n\\n---MAE VALIDATION--\\n\\n\")\n",
    "        print(\"Model:\", mae_model_test)\n",
    "        print(\"Baseline:\", mae_base_test)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
