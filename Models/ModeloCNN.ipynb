{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5089, 13, 26, 9)\n"
     ]
    }
   ],
   "source": [
    "# -Training Dataset-\n",
    "dataset = np.load('datasets/dataset_train_3.npz')\n",
    "dataset_x_or = dataset['X']\n",
    "dataset_y_or = dataset['Y']\n",
    "\n",
    "# -Validation Dataset-\n",
    "test_dataset = np.load(\"datasets/dataset_test.npz\") \n",
    "test_dataset_x = test_dataset['X']\n",
    "test_dataset_y = test_dataset['Y']\n",
    "test_dataset_y_resh = np.reshape(test_dataset_y, (-1, 1)) \n",
    "\n",
    "print(dataset_x_or.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "\n",
    "    def __init__(self, name=\"CNN\", writer_name=\"CNN\",\n",
    "                 l1_num_filt = 16, l1_window = [4,4], l1_strides = [1,1],\n",
    "                 l2_num_filt = 32, l2_window = [2,2], l2_strides = [1,1],\n",
    "                 padding_type = \"VALID\",\n",
    "                 max_pool_size = [2, 2],\n",
    "                 max_pool_str = [2, 2],\n",
    "                 fc_num_units = 256):\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # Batch of inputs (game states, one-hot encoded)\n",
    "            self.X = tf.placeholder(tf.float32, [None, 13, 26, 9], name=\"X\") # type tf.float32 is needed for the rest of operations\n",
    "\n",
    "            # Batch of outputs (correct predictions of number of actions)\n",
    "            self.Y_corr = tf.placeholder(tf.float32, [None, 1], name=\"Y\")\n",
    "            \n",
    "            # Dropout prob\n",
    "            self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "            \n",
    "            # Placeholder for batch normalization\n",
    "            # During training (big batches) -> true, during test (small batches) -> false\n",
    "            self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Batch Normalization of inputs\n",
    "            \"\"\"\n",
    "            \n",
    "            self.X_norm = tf.layers.batch_normalization(self.X, axis = 3, momentum=0.99, training=self.is_training)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            \"\"\"\n",
    "            \n",
    "            # Padding = \"VALID\" -> no padding, \"SAME\" -> padding to keep the output dimension the same as the input one\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.X_norm,\n",
    "                                         filters = l1_num_filt,\n",
    "                                         kernel_size = l1_window,\n",
    "                                         strides = l1_strides,\n",
    "                                         padding = padding_type,\n",
    "                                         activation = tf.nn.relu,\n",
    "                                         use_bias = True,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            # Flatten output of conv layers\n",
    "            \n",
    "            #self.flatten = tf.contrib.layers.flatten(self.conv1)\n",
    "            \n",
    "            # Max pooling\n",
    "            \n",
    "            self.conv1 = tf.layers.max_pooling2d(inputs = self.conv1,\n",
    "                                                pool_size = max_pool_size,\n",
    "                                                strides = max_pool_str,\n",
    "                                                padding = \"VALID\"\n",
    "                                                )\n",
    "            \n",
    "             \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\"self.conv2 = tf.layers.conv2d(inputs = self.conv1,\n",
    "                                         filters = l2_num_filt,\n",
    "                                         kernel_size = l2_window,\n",
    "                                         strides = l2_strides,\n",
    "                                         padding = padding_type,\n",
    "                                         activation = tf.nn.relu,\n",
    "                                         use_bias = True,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv2\")\n",
    "            \n",
    "            # Max pooling\n",
    "            \n",
    "            self.conv2 = tf.layers.max_pooling2d(inputs = self.conv2,\n",
    "                                                pool_size = [2, 2]\n",
    "                                                strides = [2, 2]\n",
    "                                                padding = \"VALID\",\n",
    "                                                )\"\"\"\n",
    "            \n",
    "            # Flatten output of conv layers\n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv1)\n",
    "            \n",
    "            \n",
    "            # Fully connected layer\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = fc_num_units,\n",
    "                                  activation = tf.nn.relu,\n",
    "                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  name=\"fc\")\n",
    "            \n",
    "            # Dropout\n",
    "            \n",
    "            self.fc = tf.layers.dropout(self.fc, rate=self.dropout_prob)\n",
    "            \n",
    "            # Output Layer\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 1, \n",
    "                                          activation=None)\n",
    "            \n",
    "            # Train\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.output - self.Y_corr), name=\"loss\") # Quadratic loss\n",
    "            \n",
    "            self.alfa = tf.placeholder(tf.float32)\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.alfa, name=\"optimizer\")\n",
    "            \n",
    "            # Mean and Variance Shift Operations needed for Batch Normalization\n",
    "            self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            # Execute mean and variance updates of batch norm each training step\n",
    "            with tf.control_dependencies(self.update_ops):\n",
    "                self.train_op = self.optimizer.minimize(self.loss, name=\"train_op\")\n",
    "            \n",
    "            # Summaries\n",
    "            self.train_loss_sum = tf.summary.scalar('train_loss', self.loss) # Training loss\n",
    "            self.test_loss_sum = tf.summary.scalar('test_loss', self.loss) # Validation loss\n",
    "            \n",
    "            self.writer = tf.summary.FileWriter(\"CNN_pruebas3/\" + writer_name)\n",
    "            self.writer.add_graph(tf.get_default_graph())\n",
    "            \n",
    "            \n",
    "        # Get Moving Mean and Variance of Batch Norm\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            self.get_mov_mean = tf.get_variable('batch_normalization/moving_mean')\n",
    "            self.get_mov_var = tf.get_variable('batch_normalization/moving_variance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---MAE VALIDATION--\n",
      "\n",
      "\n",
      "Model: 6.163563980444057\n",
      "Baseline: 7.093921375921377\n",
      "\n",
      "\n",
      "---MAE VALIDATION--\n",
      "\n",
      "\n",
      "Model: 6.505212569998289\n",
      "Baseline: 7.093921375921377\n",
      "\n",
      "\n",
      "---MAE VALIDATION--\n",
      "\n",
      "\n",
      "Model: 4.778104652790239\n",
      "Baseline: 7.093921375921377\n",
      "\n",
      "\n",
      "---MAE VALIDATION--\n",
      "\n",
      "\n",
      "Model: 5.6142261678522285\n",
      "Baseline: 7.093921375921377\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "# Minibatch size\n",
    "batch_size = 16 # Mejor tama√±o\n",
    "\n",
    "# Test Hyperparameters\n",
    "# out_units = [16, 32, 64, 128] # Output Units\n",
    "# num_filts = [2, 4, 8, 16] # Number of filters layer 1\n",
    "# win_sizes = [[2,2], [4,4]] # Window Size\n",
    "# strides = [[1,1], [2,2], [4,4]] # Stride Size\n",
    "# paddings = ['VALID', 'SAME']\n",
    "\n",
    "out_units = [16] # Output Units\n",
    "num_filts = [2] # Number of filters layer 1\n",
    "win_sizes = [[4,4]] # Window Size\n",
    "strides = [[2,2]] # Stride Size\n",
    "paddings = ['SAME']\n",
    "\n",
    "# Pooling\n",
    "pool_win = [2, 2]\n",
    "pool_str = [1, 1] # Overlapping Pooling\n",
    "    \n",
    "# Learning Rates\n",
    "alfas = [0.005]\n",
    "\n",
    "# Dropout probability\n",
    "drop_prob = 0.5\n",
    "\n",
    "# Different Number of Samples\n",
    "num_samples_all = [500]\n",
    "\n",
    "for num_samples in num_samples_all:\n",
    "    dataset_x = dataset_x_or[:num_samples]\n",
    "    dataset_y = dataset_y_or[:num_samples]\n",
    "    \n",
    "    \n",
    "    for num_filt in num_filts:\n",
    "        for win_size in win_sizes:\n",
    "            for stride in strides:\n",
    "                for out_unit in out_units:\n",
    "                    for num_prueba in range(1, 11):\n",
    "                    \n",
    "                    #if stride[0] <= win_size[0]: # Never use strides bigger than window size\n",
    "                    \n",
    "                        cnn_name = \"CNN\"\n",
    "                        #writer_name = \"Dataset=1000_dropout={}_Test{}_max_pool=[{},str={}]_out={}_filt={}_win={}_str={}_pad={}\".format(drop_prob, num_prueba, pool_win, pool_str, out_unit, num_filt, win_size, stride, 'SAME')\n",
    "                        writer_name = \"2_Num_samples={}, T={}\".format(num_samples, num_prueba)\n",
    "                        \n",
    "                        # Reset Default Graph\n",
    "                        tf.reset_default_graph()\n",
    "\n",
    "                        # Create CNN Architecture to test\n",
    "                        CNN_test = CNN(name=cnn_name, writer_name=writer_name,\n",
    "                                       l1_num_filt = num_filt, l1_window = win_size, l1_strides = stride,\n",
    "                                       padding_type = 'SAME',\n",
    "                                       fc_num_units = out_unit,\n",
    "                                       max_pool_size = pool_win,\n",
    "                                       max_pool_str = pool_str) \n",
    "\n",
    "                        with tf.Session() as sess:\n",
    "                            sess.run(tf.global_variables_initializer()) # Initialize all variables\n",
    "\n",
    "                            rkf = RepeatedKFold(n_splits=int(len(dataset_y) // batch_size),\n",
    "                                              n_repeats=num_epochs, random_state=41982) # Get randomized indexes for minibatches\n",
    "\n",
    "                            # Feed Dict for validation (uses validation set)\n",
    "                            data_dict_test = {CNN_test.X:test_dataset_x, CNN_test.Y_corr:test_dataset_y_resh,\n",
    "                                             CNN_test.dropout_prob:0.0, CNN_test.is_training : False, \n",
    "                                             CNN_test.alfa : 0.005}\n",
    "\n",
    "                            # Train the model\n",
    "\n",
    "                            it = 0\n",
    "\n",
    "                            for _, batch_index in rkf.split(dataset_y):\n",
    "                                batch_x = np.take(dataset_x, batch_index, axis=0) # Obtain current training batch\n",
    "                                batch_y = np.take(dataset_y, batch_index)\n",
    "                                batch_y = np.reshape(batch_y, (-1, 1))\n",
    "\n",
    "                                data_dict = {CNN_test.X:batch_x, CNN_test.Y_corr:batch_y,\n",
    "                                             CNN_test.dropout_prob:drop_prob, CNN_test.is_training : True,\n",
    "                                             CNN_test.alfa : 0.005}\n",
    "\n",
    "                                sess.run(CNN_test.train_op, feed_dict=data_dict) # Execute one training step\n",
    "                                \n",
    "                                # Print Batch Norm parameters\n",
    "                                #mean, variance = sess.run([CNN_test.get_mov_mean, CNN_test.get_mov_var])\n",
    "                                \n",
    "                                #print(\"Mean: {}, Variance: {}\".format(mean, variance))\n",
    "\n",
    "                                # Periodically check losses\n",
    "                                if it % 5 == 0:                \n",
    "                                    sum1 = sess.run(CNN_test.test_loss_sum, feed_dict=data_dict_test)\n",
    "                                    CNN_test.writer.add_summary(sum1, it)\n",
    "                                    sum2 = sess.run(CNN_test.train_loss_sum, feed_dict=data_dict)\n",
    "                                    CNN_test.writer.add_summary(sum2, it)   \n",
    "\n",
    "                                it += 1\n",
    "\n",
    "                            # ----- VALIDATION -----\n",
    "\n",
    "                            # Compute predictions\n",
    "                            test_pred = sess.run(CNN_test.output, feed_dict=data_dict_test)\n",
    "\n",
    "                            mae_base_test = mean_absolute_error(test_dataset_y, np.repeat(np.mean(dataset_y), test_dataset_y.shape[0]))\n",
    "                            mae_model_test = mean_absolute_error(test_dataset_y, test_pred)\n",
    "\n",
    "                            print(\"\\n\\n---MAE VALIDATION--\\n\\n\")\n",
    "                            print(\"Model:\", mae_model_test)\n",
    "                            print(\"Baseline:\", mae_base_test)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
